{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1db1822-d0a4-4785-857f-c642f1224e55",
   "metadata": {},
   "source": [
    "# Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9612c4-a77e-4f28-ae10-edffbd4dc1c3",
   "metadata": {},
   "source": [
    "## Missing values are common when working with real-world datasets â€“ not the cleaned ones available on Kaggle, for example. Missing data could result from a human factor (for example, a person deliberately failing to respond to a survey question), a problem in electrical sensors, or other factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6208f76e-ab94-4a52-9762-fdce1a759823",
   "metadata": {},
   "source": [
    "## Many machine learning algorithms fail if the dataset contains missing values. However, algorithms like K-nearest and Naive Bayes support data with missing values. You may end up building a biased machine learning model, leading to incorrect results if the missing values are not handled properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb7a96b-34d5-4662-bb56-99116dfceee5",
   "metadata": {},
   "source": [
    "## Many popular predictive models such as support vector machines, the glmnet, and neural networks, cannot tolerate any amount of missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b998a62-38db-4204-9d5b-6c2a0a923655",
   "metadata": {},
   "source": [
    "# Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2293a88-0647-41f2-be24-181b6d38b228",
   "metadata": {},
   "source": [
    "## There are several techniques that can be used to handle missing data, including:\n",
    "\n",
    "## Deletion: deleting the entire row or column that contains missing values.\n",
    "## Imputation: filling in missing values with estimated values based on the available data.\n",
    "## Prediction: using machine learning algorithms to predict missing values based on the available data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d09c8f-9b74-4c44-a7e2-1b9da4c9cd4c",
   "metadata": {},
   "source": [
    "## Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fae2c321-2d3a-4133-af67-482bcbcbb24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "1  2.0  6.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create a DataFrame with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4], 'B': [None, 6, 7, 8]})\n",
    "\n",
    "# drop any rows that contain missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5067af0-32e0-4409-a20d-fafa8bd24566",
   "metadata": {},
   "source": [
    "## Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32c5a293-2f6c-4959-bce3-d6f4e2bd36cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A    B\n",
      "0  1.000000  7.0\n",
      "1  2.000000  6.0\n",
      "2  2.333333  7.0\n",
      "3  4.000000  8.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create a DataFrame with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4], 'B': [None, 6, 7, 8]})\n",
    "\n",
    "# fill in missing values with the mean of the column\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8479da1f-6bb4-407c-b31f-df690c465d9c",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b61c85-013a-46f7-b463-679e1ab7ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# create a DataFrame with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4], 'B': [None, 6, 7, 8]})\n",
    "\n",
    "# split the data into two sets: one with missing values and one without\n",
    "df_missing = df[df.isnull().any(axis=1)]\n",
    "df_not_missing = df[~df.isnull().any(axis=1)]\n",
    "\n",
    "# fit a random forest regression model to the data without missing values\n",
    "model = RandomForestRegressor()\n",
    "model.fit(df_not_missing.drop('B', axis=1), df_not_missing['B'])\n",
    "\n",
    "# predict the missing values\n",
    "predicted_values = model.predict(df_missing.drop('B', axis=1))\n",
    "\n",
    "# fill in the missing values with the predicted values\n",
    "df.loc[df['B'].isnull(), 'B'] = predicted_values\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c7df8b-2e8b-4588-8e7d-b4230dbe01e2",
   "metadata": {},
   "source": [
    "# Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387f9b4-5577-4edd-8acf-3dfc6be4be93",
   "metadata": {},
   "source": [
    "## Imbalanced data refers to a situation where the distribution of classes in a dataset is unequal, with one or more classes being represented by significantly fewer samples than the other classes. This is a common problem in many real-world scenarios such as fraud detection, disease diagnosis, and customer churn prediction.\n",
    "\n",
    "## If imbalanced data is not handled, it can lead to several issues, including:\n",
    "\n",
    "## 1. Poor performance of machine learning models: In an imbalanced dataset, the minority class is often underrepresented in the training set, leading to biased models that tend to predict the majority class more often than the minority class. This can result in poor performance metrics such as accuracy, precision, and recall.\n",
    "\n",
    "## 2. Overfitting: When a machine learning model is trained on imbalanced data, it may overfit to the majority class, leading to poor generalization performance on new data.\n",
    "\n",
    "## 3. Misleading evaluation metrics: If the imbalanced dataset is used to evaluate a model's performance, metrics such as accuracy can be misleading. For example, a classifier that always predicts the majority class in a dataset with a class distribution of 99:1 would have an accuracy of 99%, even though it has not learned anything useful.\n",
    "\n",
    "## To handle imbalanced data, several techniques can be used, including:\n",
    "\n",
    "## 1. Resampling: This involves either oversampling the minority class or undersampling the majority class to balance the class distribution.\n",
    "\n",
    "## 2. Synthetic data generation: This involves creating new synthetic samples for the minority class using techniques such as SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "\n",
    "## 3. Cost-sensitive learning: This involves assigning different misclassification costs to different classes during model training to give more weight to the minority class.\n",
    "\n",
    "## 4 Algorithmic ensemble methods: This involves combining multiple models to create a stronger classifier that is better able to handle imbalanced data.\n",
    "\n",
    "## In summary, imbalanced data can cause problems such as poor model performance, overfitting, and misleading evaluation metrics if not handled properly. To address these issues, various techniques can be used to balance the class distribution in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc09dc6-b44e-4d9e-9eaf-b703ce5b9c89",
   "metadata": {},
   "source": [
    "# Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c589dbbb-7803-4425-8c3e-e9d0c536345f",
   "metadata": {},
   "source": [
    "## Up-sampling and down-sampling are two techniques used to balance the class distribution in imbalanced datasets.\n",
    "\n",
    "## Up-sampling involves increasing the number of samples in the minority class to match the number of samples in the majority class. This can be done by either duplicating existing samples or generating new synthetic samples using techniques such as SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "\n",
    "## Down-sampling involves decreasing the number of samples in the majority class to match the number of samples in the minority class. This can be done by randomly removing samples from the majority class.\n",
    "\n",
    "## When to use Up-sampling and Down-sampling:\n",
    "\n",
    "## Up-sampling is generally used when the number of samples in the minority class is significantly lower than the number of samples in the majority class. For example, consider a dataset for fraud detection where only 1% of the transactions are fraudulent. In this case, up-sampling can be used to increase the number of fraudulent transactions in the dataset to improve the performance of the model.\n",
    "\n",
    "## Down-sampling is generally used when the number of samples in the majority class is significantly higher than the number of samples in the minority class. For example, consider a dataset for cancer diagnosis where only 10% of the samples are cancerous. In this case, down-sampling can be used to reduce the number of non-cancerous samples in the dataset to balance the class distribution and prevent the model from being biased towards the majority class.\n",
    "\n",
    "## Here's an example of up-sampling using the Python library scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8608c47b-385a-4c23-b437-6900355acf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature  class\n",
      "0        1      0\n",
      "1        2      0\n",
      "2        3      0\n",
      "3        4      0\n",
      "4        5      0\n",
      "5        6      0\n",
      "6        7      0\n",
      "9       10      1\n",
      "8        9      1\n",
      "9       10      1\n",
      "9       10      1\n",
      "7        8      1\n",
      "9       10      1\n",
      "9       10      1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "\n",
    "# create an imbalanced dataset\n",
    "df = pd.DataFrame({'feature': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "                   'class': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1]})\n",
    "\n",
    "# separate majority and minority classes\n",
    "df_majority = df[df['class']==0]\n",
    "df_minority = df[df['class']==1]\n",
    "\n",
    "# up-sample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=7,      # to match majority class size\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "# combine majority class with up-sampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "print(df_upsampled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb002732-b2c0-4e26-a0c3-55ab67c6ace8",
   "metadata": {},
   "source": [
    "## Here's an example of down-sampling using the Python library scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec46b4a6-74cb-4865-9dfb-0182670cd5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature  class\n",
      "1        2      0\n",
      "3        4      0\n",
      "5        6      1\n",
      "6        7      1\n",
      "7        8      1\n",
      "8        9      1\n",
      "9       10      1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "\n",
    "# create an imbalanced dataset\n",
    "df = pd.DataFrame({'feature': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "                   'class': [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]})\n",
    "\n",
    "# separate majority and minority classes\n",
    "df_majority = df[df['class']==0]\n",
    "df_minority = df[df['class']==1]\n",
    "\n",
    "# down-sample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                   replace=False,    # sample without replacement\n",
    "                                   n_samples=2,      # to match minority class size\n",
    "                                   random_state=123) # reproducible results\n",
    "\n",
    "# combine majority class with down-sampled minority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "print(df_downsampled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba6de9c-c1d7-42b2-bf6a-cd098dffd1f0",
   "metadata": {},
   "source": [
    "# Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb53320-254c-42bf-a42c-dacfcd7afd60",
   "metadata": {},
   "source": [
    "## Data augmentation is a technique used to increase the size of a dataset by creating new synthetic samples from the existing ones. The idea behind data augmentation is to introduce diversity into the dataset, which can improve the performance of machine learning models by reducing overfitting and improving generalization.\n",
    "\n",
    "## SMOTE (Synthetic Minority Over-sampling Technique) is a popular data augmentation technique used to balance the class distribution in imbalanced datasets. The SMOTE algorithm works by creating synthetic samples in the minority class by interpolating between existing samples.\n",
    "\n",
    "## Here's how the SMOTE algorithm works:\n",
    "\n",
    "## For each sample in the minority class, SMOTE selects k-nearest neighbors from the same class (k is a parameter that can be tuned).\n",
    "## SMOTE then generates new synthetic samples by interpolating between the selected sample and its k-nearest neighbors. Specifically, SMOTE randomly selects a neighbor and computes the difference between the feature values of the selected sample and the neighbor. SMOTE then multiplies this difference by a random number between 0 and 1 and adds the result to the selected sample to create a new synthetic sample.\n",
    "## This process is repeated for each sample in the minority class until the desired level of balance is achieved.\n",
    "## SMOTE can be implemented using various libraries in Python, such as imbalanced-learn and scikit-learn. Here's an example of how to use SMOTE with imbalanced-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298e91e4-7871-479e-ac6b-50a090d0fc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "# create an imbalanced dataset\n",
    "df = pd.DataFrame({'feature_1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "                   'feature_2': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "                   'class': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1]})\n",
    "\n",
    "# separate features and target\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# apply SMOTE\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# combine resampled features and target\n",
    "df_resampled = pd.concat([pd.DataFrame(X_resampled), pd.DataFrame(y_resampled, columns=['class'])], axis=1)\n",
    "\n",
    "print(df_resampled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e193cd6-73c7-48b9-9a21-23b9dc1d9526",
   "metadata": {},
   "source": [
    "# Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04d7378-ec48-4d94-9d81-54889c903dd4",
   "metadata": {},
   "source": [
    "## Outliers in a dataset are data points that are significantly different from the other data points in the dataset. Outliers can occur for various reasons, such as measurement errors, data entry errors, or natural variation in the data.\n",
    "\n",
    "## Handling outliers is essential because they can have a significant impact on the performance of machine learning models. Outliers can skew the distribution of the data, leading to biased model predictions. Outliers can also have a disproportionate effect on model training, causing the model to overfit the outliers and underfit the rest of the data.\n",
    "## Here are some reasons why it's essential to handle outliers in a dataset:\n",
    "\n",
    "## 1. Outliers can affect the statistical properties of the data. For example, the mean and standard deviation of a dataset can be heavily influenced by the presence of outliers. This can lead to incorrect conclusions and predictions.\n",
    "\n",
    "## 2. Outliers can affect the performance of machine learning models. Outliers can cause models to be biased towards the outlier values, leading to poor generalization on new data.\n",
    "\n",
    "## 3. Outliers can cause numerical instability in some algorithms. For example, some optimization algorithms may fail to converge in the presence of outliers.\n",
    "\n",
    "## There are various methods for handling outliers in a dataset. One approach is to remove the outliers from the dataset. Another approach is to replace the outliers with a more reasonable value, such as the mean or median of the non-outlier values. Alternatively, some machine learning algorithms can handle outliers implicitly, such as robust regression methods.\n",
    "## In summary, handling outliers is essential to ensure that machine learning models are trained on representative data and can generalize to new data accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e861221f-18bc-4612-91a5-b2c54122bc08",
   "metadata": {},
   "source": [
    "## There are several techniques that can be used to handle missing data in a dataset, some of which are:\n",
    "\n",
    "## 1. Deletion: In this technique, rows or columns with missing data are removed entirely from the dataset. This approach is useful when the missing data is relatively small compared to the overall size of the dataset, and the analysis can still be performed without the missing data. However, it can result in a loss of valuable information and reduce the representativeness of the dataset.\n",
    "## 2. Imputation: In this technique, the missing data is estimated or imputed based on the available data. There are several methods for imputing missing data, including mean imputation, median imputation, regression imputation, and k-nearest neighbor imputation. The choice of imputation method depends on the nature of the data and the analysis being performed.\n",
    "## 3. Machine learning methods: Another approach is to use machine learning methods to predict missing values based on the available data. This approach can be particularly useful when there is a large amount of missing data or when the missing data is related to other variables in the dataset.\n",
    "## 4. Multiple imputation: Multiple imputation involves creating multiple imputed datasets and analyzing them separately. The results from each dataset are then combined to obtain an overall estimate. This approach can account for the uncertainty associated with missing data imputation and provide more robust estimates.\n",
    "\n",
    "## It's important to note that the choice of missing data handling technique depends on the nature of the missing data and the analysis being performed. It's also important to carefully evaluate the impact of the missing data handling technique on the analysis results and report any limitations or assumptions made during the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f67652-e93a-428d-8164-bfba1ab3a1f8",
   "metadata": {},
   "source": [
    "# Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a099ce-4f28-4595-8c10-7cdc62e18341",
   "metadata": {},
   "source": [
    "## There are several strategies that can be used to determine if the missing data is missing at random (MAR) or if there is a pattern to the missing data. Some of these strategies are:\n",
    "\n",
    "## 1. Visual inspection: One way to detect patterns in missing data is to visualize the missing data using heatmaps or other graphical representations. If there is a pattern in the missing data, it may be visible in the visual representation.\n",
    "\n",
    "## 2. Statistical tests: There are several statistical tests that can be used to determine if the missing data is missing at random or not. For example, Little's MCAR test, which tests the hypothesis that the missing data is completely at random, can be used to determine if the missing data is MAR or not.\n",
    "\n",
    "## 3. Imputation and comparison: Another approach is to impute the missing data using different imputation methods and compare the results. If the results are consistent across different imputation methods, it may suggest that the missing data is missing at random.\n",
    "\n",
    "## 4. Subgroup analysis: It may also be useful to perform subgroup analysis to determine if there is a pattern in the missing data based on certain variables or groups. For example, if the missing data is more common in one particular demographic group, it may suggest that the missing data is not MAR.\n",
    "\n",
    "## 5. Expert knowledge: Sometimes, expert knowledge of the domain can help to determine if there is a pattern in the missing data. For example, if missing data is related to a particular time period, it may suggest that there was a change in data collection procedures during that period.\n",
    "\n",
    "## In summary, detecting patterns in missing data is important to ensure that the missing data handling techniques are appropriate for the analysis being performed. A combination of visual inspection, statistical tests, imputation, subgroup analysis, and expert knowledge can be used to determine if the missing data is missing at random or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c27d20-118d-488b-b381-18e212a8a924",
   "metadata": {},
   "source": [
    "# Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dae3ca4-86bc-4797-92e6-a344ca0147d3",
   "metadata": {},
   "source": [
    "## When working with imbalanced datasets, where one class is significantly more prevalent than the other, traditional evaluation metrics such as accuracy can be misleading. Here are some strategies to evaluate the performance of machine learning models on imbalanced datasets:\n",
    "\n",
    "## 1. Confusion matrix: A confusion matrix can be used to visualize the performance of the machine learning model. It shows the number of true positives, true negatives, false positives, and false negatives. From this matrix, other evaluation metrics such as precision, recall, and F1 score can be calculated.\n",
    "\n",
    "## 2. Precision and recall: Precision and recall are evaluation metrics that are more appropriate for imbalanced datasets. Precision measures the percentage of true positives out of all positive predictions, while recall measures the percentage of true positives out of all actual positives. Precision and recall can be combined into a single F1 score, which is the harmonic mean of precision and recall.\n",
    "\n",
    "## 3. ROC curve and AUC: Receiver operating characteristic (ROC) curve and the area under the curve (AUC) are also useful evaluation metrics for imbalanced datasets. The ROC curve plots the true positive rate against the false positive rate at different classification thresholds. The AUC represents the area under the ROC curve, with a value of 1 indicating a perfect classifier and a value of 0.5 indicating a random classifier.\n",
    "\n",
    "## 4. Class weights and resampling techniques: Class weights can be used to adjust the weight given to each class during training to account for the class imbalance. Resampling techniques such as oversampling the minority class (e.g., using SMOTE) or undersampling the majority class can also be used to balance the classes.\n",
    "\n",
    "## 5. Cross-validation: Cross-validation can be used to assess the model's performance on multiple folds of the data, which can provide a more reliable estimate of the model's performance.\n",
    "\n",
    "## In summary, evaluating the performance of machine learning models on imbalanced datasets requires special attention. A combination of evaluation metrics such as confusion matrix, precision, recall, F1 score, ROC curve, AUC, class weights, resampling techniques, and cross-validation can be used to accurately evaluate the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71374fee-0ad4-4b4a-889d-3bd8d4981e7f",
   "metadata": {},
   "source": [
    "# Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd4cd5fc-07e8-4f6c-87c2-96417489e82c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'satisfaction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Separate majority and minority classes\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df_majority \u001b[38;5;241m=\u001b[39m df[\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msatisfaction\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msatisfied\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m df_minority \u001b[38;5;241m=\u001b[39m df[df\u001b[38;5;241m.\u001b[39msatisfaction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdissatisfied\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Downsample majority class\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5895\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5896\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5897\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5898\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5899\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5900\u001b[0m ):\n\u001b[1;32m   5901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'satisfaction'"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "# Separate majority and minority classes\n",
    "df_majority = df[df.satisfaction == 'satisfied']\n",
    "df_minority = df[df.satisfaction == 'dissatisfied']\n",
    "\n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                   replace=False,    # sample without replacement\n",
    "                                   n_samples=len(df_minority), # match minority class size\n",
    "                                   random_state=42)  # reproducible results\n",
    "\n",
    "# Combine minority class and downsampled majority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "# Separate majority and minority classes\n",
    "X = df.drop('satisfaction', axis=1)\n",
    "y = df['satisfaction']\n",
    "\n",
    "# Generate centroids for majority class\n",
    "cc = ClusterCentroids(random_state=42)\n",
    "X_resampled, y_resampled = cc.fit_resample(X, y)\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "# Separate majority and minority classes\n",
    "X = df.drop('satisfaction', axis=1)\n",
    "y = df['satisfaction']\n",
    "\n",
    "# Remove samples using Tomek links\n",
    "tl = TomekLinks()\n",
    "X_resampled, y_resampled = tl.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e3ce37-7946-4005-adf6-a7e4739905cd",
   "metadata": {},
   "source": [
    "# Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3645ea6b-76a9-4af5-910e-1b9d332a89c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'occurrence'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resample\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Separate majority and minority classes\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df_majority \u001b[38;5;241m=\u001b[39m df[\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moccurrence\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m df_minority \u001b[38;5;241m=\u001b[39m df[df\u001b[38;5;241m.\u001b[39moccurrence \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Upsample minority class\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5895\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5896\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5897\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5898\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5899\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5900\u001b[0m ):\n\u001b[1;32m   5901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'occurrence'"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = df[df.occurrence == 0]\n",
    "df_minority = df[df.occurrence == 1]\n",
    "\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=len(df_majority), # match majority class size\n",
    "                                 random_state=42)  # reproducible results\n",
    "\n",
    "# Combine majority class and upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('occurrence', axis=1)\n",
    "y = df['occurrence']\n",
    "\n",
    "# Generate synthetic samples for minority class using SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = sm.fit_resample(X, y)\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('occurrence', axis=1)\n",
    "y = df['occurrence']\n",
    "\n",
    "# Generate synthetic samples for minority class using ADASYN\n",
    "ada = ADASYN(random_state=42)\n",
    "X_resampled, y_resampled = ada.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0805a51-295a-4202-9c23-4dd68d99164f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
